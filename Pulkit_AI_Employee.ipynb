{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\" 1. Data Processing Module \""
      ],
      "metadata": {
        "id": "d6qcxTcvD1ht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\" 1a. Data Ingestion \""
      ],
      "metadata": {
        "id": "x1CFY-oyEATk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "def load_data(file_path):\n",
        "    if file_path.endswith('.csv'):\n",
        "        return pd.read_csv(file_path)\n",
        "    elif file_path.endswith('.json'):\n",
        "        with open(file_path, 'r') as file:\n",
        "            data = json.load(file)\n",
        "        return pd.DataFrame(data)\n",
        "    elif file_path.endswith('.xlsx') or file_path.endswith('.xls'):\n",
        "        return pd.read_excel(file_path)\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file format. Please provide a CSV, JSON, or Excel file.\")"
      ],
      "metadata": {
        "id": "dyYKq5SaxsNu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\" 1b. Data Pre-Processing (& cleaning)\""
      ],
      "metadata": {
        "id": "bdqRZ-i8EeOF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "\n",
        "def preprocess_data(df):\n",
        "    # Handling missing values by filling them with the mean of the column\n",
        "    df.fillna(df.mean(), inplace=True)\n",
        "\n",
        "    # Encoding categorical variables\n",
        "    for column in df.select_dtypes(include=['object']).columns:\n",
        "        le = LabelEncoder()\n",
        "        df[column] = le.fit_transform(df[column])\n",
        "\n",
        "    # Normalizing numerical features\n",
        "    scaler = StandardScaler()\n",
        "    df[df.select_dtypes(include=['float64', 'int64']).columns] = scaler.fit_transform(df.select_dtypes(include=['float64', 'int64']))\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "ongkGJ7lxy6s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\" 2. Analysis Engine \""
      ],
      "metadata": {
        "id": "-_82tRstEoLM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "def analyze_data(df):\n",
        "    results = {}\n",
        "\n",
        "    # 1. Trend Analysis using Linear Regression\n",
        "    for column in df.select_dtypes(include=[np.number]).columns:\n",
        "        X = np.array(range(len(df))).reshape(-1, 1)\n",
        "        y = df[column].values.reshape(-1, 1)\n",
        "        lr = LinearRegression()\n",
        "        lr.fit(X, y)\n",
        "        trend = lr.coef_[0][0]\n",
        "        results[f'{column}_trend'] = trend\n",
        "\n",
        "    # 2. Clustering to identify patterns using KMeans\n",
        "    kmeans = KMeans(n_clusters=3)\n",
        "    clusters = kmeans.fit_predict(df)\n",
        "    df['Cluster'] = clusters\n",
        "    results['clusters'] = df['Cluster'].value_counts().to_dict()\n",
        "\n",
        "    # 3. Feature Importance using RandomForestClassifier (Assuming 'target' is the column to predict)\n",
        "    if 'target' in df.columns:\n",
        "        X = df.drop('target', axis=1)\n",
        "        y = df['target']\n",
        "        rf = RandomForestClassifier()\n",
        "        rf.fit(X, y)\n",
        "        feature_importances = rf.feature_importances_\n",
        "        results['feature_importances'] = dict(zip(X.columns, feature_importances))\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "_VmsSMAhyBO7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\" 3. Report Generation \""
      ],
      "metadata": {
        "id": "e-9hx7S_E04F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "def generate_report(df, analysis_results, report_name=\"report\"):\n",
        "\n",
        "    # Creating a summary report as a text file\n",
        "    with open(f\"{report_name}.txt\", \"w\") as report:\n",
        "        report.write(\"Data Analysis Report\\n\")\n",
        "        report.write(\"=\"*50 + \"\\n\\n\")\n",
        "\n",
        "        # Summary of trends\n",
        "        report.write(\"Trends Summary:\\n\")\n",
        "        for key, value in analysis_results.items():\n",
        "            if 'trend' in key:\n",
        "                report.write(f\"{key}: {'increasing' if value > 0 else 'decreasing' if value < 0 else 'stable'}\\n\")\n",
        "\n",
        "        report.write(\"\\n\")\n",
        "\n",
        "        # Summary of clustering\n",
        "        if 'clusters' in analysis_results:\n",
        "            report.write(\"Cluster Summary:\\n\")\n",
        "            for cluster, count in analysis_results['clusters'].items():\n",
        "                report.write(f\"Cluster {cluster}: {count} instances\\n\")\n",
        "\n",
        "        report.write(\"\\n\")\n",
        "\n",
        "        # Summary of feature importance\n",
        "        if 'feature_importances' in analysis_results:\n",
        "            report.write(\"Feature Importance Summary:\\n\")\n",
        "            sorted_importances = sorted(analysis_results['feature_importances'].items(), key=lambda x: x[1], reverse=True)\n",
        "            for feature, importance in sorted_importances:\n",
        "                report.write(f\"{feature}: {importance:.4f}\\n\")\n",
        "\n",
        "    # Visualizing trends using line plots\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    for column in df.select_dtypes(include=[float, int]).columns:\n",
        "        plt.plot(df[column], label=column)\n",
        "    plt.title('Trends in Numerical Data')\n",
        "    plt.legend()\n",
        "    plt.savefig(f\"{report_name}_trends.png\")\n",
        "    plt.show()\n",
        "\n",
        "    # Visualizing clusters using pair plots (if applicable)\n",
        "    if 'Cluster' in df.columns:\n",
        "        sns.pairplot(df, hue=\"Cluster\", palette=\"Set2\")\n",
        "        plt.savefig(f\"{report_name}_clusters.png\")\n",
        "        plt.show()\n",
        "\n",
        "    # Visualizing feature importance using a bar plot\n",
        "    if 'feature_importances' in analysis_results:\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        sns.barplot(x=list(analysis_results['feature_importances'].keys()), y=list(analysis_results['feature_importances'].values()))\n",
        "        plt.title('Feature Importances')\n",
        "        plt.xticks(rotation=45, ha='right')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f\"{report_name}_feature_importances.png\")\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "ckzYB8hQyQRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\" 4. UI (CLI & NLP)\""
      ],
      "metadata": {
        "id": "UZbMwjtlE8EF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "class AIEmployee:\n",
        "    def __init__(self, data_file):\n",
        "        self.data = load_data(data_file)\n",
        "        self.analysis_results = None\n",
        "\n",
        "    def analyze(self):\n",
        "        self.analysis_results = analyze_data(self.data)\n",
        "        print(\"Data analysis completed.\")\n",
        "\n",
        "    def generate_report(self, report_name=\"report\"):\n",
        "        if self.analysis_results:\n",
        "            generate_report(self.data, self.analysis_results, report_name)\n",
        "            print(f\"Report '{report_name}' generated successfully.\")\n",
        "        else:\n",
        "            print(\"Please run the analysis first.\")\n",
        "\n",
        "    def handle_query(self, query):\n",
        "        # Basic NLP to understand user queries\n",
        "        if re.search(r'trend|trends', query, re.I):\n",
        "            self.display_trends()\n",
        "        elif re.search(r'cluster|clusters', query, re.I):\n",
        "            self.display_clusters()\n",
        "        elif re.search(r'feature importance', query, re.I):\n",
        "            self.display_feature_importance()\n",
        "        elif re.search(r'analyze', query, re.I):\n",
        "            self.analyze()\n",
        "        elif re.search(r'report', query, re.I):\n",
        "            report_name = re.search(r'report\\s+(\\w+)', query, re.I)\n",
        "            report_name = report_name.group(1) if report_name else \"report\"\n",
        "            self.generate_report(report_name)\n",
        "        else:\n",
        "            print(\"Sorry, I didn't understand that. Please try again.\")\n",
        "\n",
        "    def display_trends(self):\n",
        "        if self.analysis_results:\n",
        "            print(\"Trends Summary:\")\n",
        "            for key, value in self.analysis_results.items():\n",
        "                if 'trend' in key:\n",
        "                    print(f\"{key}: {'increasing' if value > 0 else 'decreasing' if value < 0 else 'stable'}\")\n",
        "        else:\n",
        "            print(\"Please run the analysis first.\")\n",
        "\n",
        "    def display_clusters(self):\n",
        "        if self.analysis_results and 'clusters' in self.analysis_results:\n",
        "            print(\"Cluster Summary:\")\n",
        "            for cluster, count in self.analysis_results['clusters'].items():\n",
        "                print(f\"Cluster {cluster}: {count} instances\")\n",
        "        else:\n",
        "            print(\"No clusters found or analysis not run yet.\")\n",
        "\n",
        "    def display_feature_importance(self):\n",
        "        if self.analysis_results and 'feature_importances' in self.analysis_results:\n",
        "            print(\"Feature Importance Summary:\")\n",
        "            sorted_importances = sorted(self.analysis_results['feature_importances'].items(), key=lambda x: x[1], reverse=True)\n",
        "            for feature, importance in sorted_importances:\n",
        "                print(f\"{feature}: {importance:.4f}\")\n",
        "        else:\n",
        "            print(\"No feature importance data found or analysis not run yet.\")"
      ],
      "metadata": {
        "id": "TWyQMdBYyi7g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\" 5. Documentation & Testing \""
      ],
      "metadata": {
        "id": "yn8wKvxFFApq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import unittest\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "class TestAIEmployee(unittest.TestCase):\n",
        "\n",
        "    def setUp(self):\n",
        "        # Sample data for testing\n",
        "        self.df = pd.DataFrame({\n",
        "            'Country': ['A', 'B', 'C'],\n",
        "            'Gold': [1, 2, 3],\n",
        "            'Silver': [2, 1, 3],\n",
        "            'Bronze': [3, 3, 1],\n",
        "            'Total': [6, 6, 7]\n",
        "        })\n",
        "\n",
        "    def test_linear_regression(self):\n",
        "        lr_model = LinearRegression().fit(self.df[['Gold', 'Silver', 'Bronze']], self.df['Total'])\n",
        "        self.assertEqual(len(lr_model.coef_), 3)\n",
        "\n",
        "    def test_kmeans_clustering(self):\n",
        "        kmeans = KMeans(n_clusters=2, random_state=0, n_init=10)\n",
        "        clusters = kmeans.fit_predict(self.df[['Gold', 'Silver', 'Bronze']])\n",
        "        self.assertEqual(len(set(clusters)), 2)\n",
        "\n",
        "    def test_random_forest(self):\n",
        "        rf_model = RandomForestRegressor(random_state=0).fit(self.df[['Gold', 'Silver', 'Bronze']], self.df['Total'])\n",
        "        self.assertEqual(len(rf_model.feature_importances_), 3)\n",
        "\n",
        "unittest.main(argv=['first-arg-is-ignored'], exit=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z05a6dgay2i8",
        "outputId": "0f746d37-9fb4-47ea-a452-005d9f5a00b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "...\n",
            "----------------------------------------------------------------------\n",
            "Ran 3 tests in 0.297s\n",
            "\n",
            "OK\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<unittest.main.TestProgram at 0x77fe27406920>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    }
  ]
}